{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "WORD2VEC MODEL\n",
    "https://www.youtube.com/watch?v=ouzJPlO64Dg&feature=youtu.be\n",
    "\n",
    "Links : How does word2vec internally work? \n",
    "https://radimrehurek.com/gensim/models/word2vec.html\n",
    "https://arxiv.org/pdf/1301.3781.pdf\n",
    "https://arxiv.org/abs/1310.4546\n",
    "\n",
    "-google pretrained model\n",
    "-50 Billion Words\n",
    "-similar context words have similar vectors\n",
    "- euclidian Distance/Similarity measured by Cosine Similarity/Distance\n",
    "\n",
    "Applications:\n",
    "-text similarity\n",
    "-language translation\n",
    "-find odd words out\n",
    "-word analogies\n",
    "\n",
    "WordEmbeddings : \n",
    "-numerical representation of words in form of vectors\n",
    "-how to use pre-trained word2vec model\n",
    "-NLP package used : Gensim (NLP package)\n",
    "\n",
    "Gensim's Word2Vec model = CBOW Model + SkipGram Model\n",
    "\n",
    "models.word2vec -- Word2vec embeddings\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "# gensim also has Doc2vec,Word2vec,FastText\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "# KeyedVectors - object containing the mapping between words and embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=word_vectors[\"anger\"]\n",
    "b=word_vectors[\"fury\"]\n",
    "cosine_similarity([a],[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     ODD ONE OUT - Min Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute the odd one out \n",
    "# find the mean of all the vectors\n",
    "# find the mean\n",
    "# find the distance of each vector from the mean\n",
    "def OddOneOut(word_list):\n",
    "    word_embedd_vectors = [word_vectors[word] for word in word_list]\n",
    "    vec_mean = np.mean(word_embedd_vectors,axis=0)\n",
    "    Odd_elem=None\n",
    "    Min_Similarity = 1.0\n",
    "    \n",
    "    for index in range(len(word_embedd_vectors)):\n",
    "        similarity = cosine_similarity([word_embedd_vectors[index]],[vec_mean])\n",
    "        print(\"Similarity between {} and mean vector is {}\".format(word_list[index],similarity))\n",
    "        if similarity<Min_Similarity:\n",
    "            Min_Similarity=similarity\n",
    "            Odd_elem=word_list[index]\n",
    "    \n",
    "    return Odd_elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = [\"apple\",\"mango\",\"juice\",\"party\",\"orange\"] \n",
    "input_2 = [\"music\",\"dance\",\"sleep\",\"dancer\",\"food\"]        \n",
    "input_3  = [\"match\",\"player\",\"football\",\"cricket\",\"dancer\"]\n",
    "input_4 = [\"india\",\"paris\",\"russia\",\"france\",\"germany\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [input_1,input_2,input_3,input_4]\n",
    "for input in inputs:\n",
    "    print(\"\\n\\nInput : \",input,\"\\n\")\n",
    "    print(\"\\nOdd One Out : \",OddOneOut(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD ANALOGIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a:b c:?\n",
    "# Let ? = d\n",
    "# so a-b = c-d is the relation --> \n",
    "# so find the similarity of a-b and look for d with similarity with same similarity\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#find the value of d\n",
    "def WordAnalogy(a,b,c,word_vectors):\n",
    "    start_time = time.time()\n",
    "    pred_d = None\n",
    "    a,b,c = a.lower(),b.lower(),c.lower()\n",
    "    vec_a,vec_b,vec_c=word_vectors[a],word_vectors[b],word_vectors[c]\n",
    "    word_list = word_vectors.vocab.keys()\n",
    "    \n",
    "    max_similarity = -100000 \n",
    "    \n",
    "    for word in word_list:\n",
    "        if word in [a,b,c]:\n",
    "            continue \n",
    "        word_vec = word_vectors[word]\n",
    "        similarity = cosine_similarity([vec_a - vec_b],[vec_c - word_vec])\n",
    "        \n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            pred_d = word\n",
    "    \n",
    "    print(\"\\nThe Max Similarity : \",max_similarity,\"\\n\")\n",
    "    end_time = time.time()\n",
    "    print(\"\\nExecution Time : {}\\n\\n\".format(end_time - start_time))\n",
    "    return pred_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1=(\"man\",\"code\",\"woman\")\n",
    "input_2=(\"italy\",\"italian\",\"india\")\n",
    "input_3=(\"india\",\"country\",\"newyork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [input_1,input_2,input_3]\n",
    "for input in inputs:\n",
    "    print(\"Predicted Value = \",WordAnalogy(*input,word_vectors),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WORD ANALOGY -MOST SIMILAR METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=['india','country'],negative=['delhi'],topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
